---
title: "Introduction-RobSparseMVA-CCA"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction-RobSparseMVA-CCA}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(RobSparseMVA)
library(mvtnorm)
```

This file contains notes and examples for the usage of the function `ccaMM()` in the R package `RobSparseMVA` (Robust and Sparse Multivariate Analysis). 

## Introduction

The function `ccaMM()` is the heart of the package, performing robust and sparse CCA (Canonical Correlation Analysis) based on an MM-algorithm (Method of Multipliers) in combination with gradient-descent optimization.

Let $  x$ and $  y$ be $p$- and $q$-dimensional random variables. The first canonical correlation coefficient $\rho_1$ and the first pair of canonical vectors $(  a_1,   b_1)$ are defined via the maximization of the Pearson correlation coefficient between linear combinations of sets of variables:
$$
\begin{aligned}
    \rho_1 &= \max_{\substack{  a,   b \\ \|  a\|= \|  b\|=1} } \text{Corr}(  a'   x,   b'   y)  \\ 
    (  a_1,   b_1)&= \text{argmax}_{\|  a\|=1,\|  b\|=1} \text{Corr}(  a'   x,   b'   y)
\end{aligned}
$$
The $k$-th canonical correlation coefficient $\rho_k$ and the respective pair of canonical vectors $(  a_k,   b_k)$ maximize the equation above under the condition that they are uncorrelated with the previous $1, \ldots, k-1$ directions.

For higher-order directions $(  a_k,   b_k)$: condition of uncorrelatednes:
\begin{equation}
      a_k' \text{Cov}(  x)   a_i = 0 \text{~~~and~~~}   b_k' \text{Cov}(  y)   b_i = 0, \mbox{ for }~i = 1, \ldots , k-1. 
\end{equation}

For a sparse setting, add penalty terms as further constraints:
\begin{equation}
    P_{  a_k}(  a_k)\leq c_1 \text{~~~and~~~} P_{  a_k}(  b_k)\leq c_2 
\end{equation}

The method and algorithm is described in detail in the paper:
Pfeiffer, P., Alfons, A. \& Filzmoser, P. (2023+). Efficient Computation of Sparse and Robust Maximum Association Estimators. [Preprint](https://arxiv.org/pdf/2311.17563.pdf)

In a more human language, we are trying to find linear combinations of two data matrices, such that the correlation between the resulting vectors is maximized. Additionally, we want the computation be robust in the presence of outliers, and be able to promote sparsity in the resulting directions. Especially in the high-dimensional case, when a lot of variables are present, this enhances interpretability. 

## A first example

Let us create a simple simulated dataset first. The two data matrices `x` and `y` follow a joint multivariate normal distribution, and the covariance structure is chosen such that the true canonical directions are sparse.
```{r}
set.seed(123)
    p <- 10
    q <- 10
    n <- 100

    cov_xx <- matrix(0, ncol = p, nrow = p)
    cov_yy <- matrix(0, ncol = q, nrow = q)
    cov_xy <- matrix(0, nrow = p, ncol = q)

    diag(cov_xx) <- 1
    diag(cov_yy) <- 1
    cov_xy <- matrix(0, nrow = p, ncol = q)
    cov_xy[1, 1] <- 0.9
    cov_xy[2, 2] <- 0.7
    
    sigma <- rbind(
    cbind(cov_xx, cov_xy),
    cbind(Matrix::t(cov_xy), cov_yy)
    )
    
    data <- rmvnorm(floor(n),
      mean = rep(0, p + q),
      sigma = sigma, checkSymmetry = F
    )
    
    x <-  as.matrix(data[, 1:p])
    y <-  as.matrix(data[, (p + 1):(p + q)])
```

Now we have the data, let us look what else we need as input for our function:
```{r}
?ccaMM
```

## Compute CCA using sample covariance
Set the elastic net parameters to 0 (Ridge penalty) and the penalties to the maximum (square root of dimension if standardization can be assumed).
```{r}
load("../data/res_classic.rda") # result can be generated by running code below
# n_dir <- 2 # we want to derive the first two directions
# res_classic <- ccaMM(x, y,
#                      k = n_dir,
#                      alpha_x = rep(0, n_dir),
#                      alpha_y = rep(0, n_dir))
plot(res_classic$a[,1], type = "l")
```

Now we can change the elastic net parameters to 1 (LASSO penalty) and leave the penalties as the default values. This means that the function will perform an automatic hyperparameter optimization for the selection of the sparsity parameters.

```{r}
load("../data/res_sparse.rda") # result can be generated by running code below
# res_sparse <- ccaMM(x, y, 
#                      k = n_dir,
#                      alpha_x = rep(1, n_dir), # elastic net parameter
#                      alpha_y = rep(1, n_dir))

```

For a robust version, we can use different options for the `method` argument of the function: The default is set to `Pearson`, resulting in CCA based on the sample covariance.
Other available options are: `Spearman`, `Kendall`, `MCD`, `MRCD`, `OGK`, `pairhuber`, `quadrant`, `Ledoit-Wolf`.

Using the optimal penalties from before we can compare the result with a robust estimation using the Spearman estimator:

```{r}
load("../data/res_spearman.rda") # result can be generated by running code below
# res_spearman <- ccaMM(x, y, 
#                      k = n_dir,
#                      method = "Spearman",
#                      alpha_x = rep(1, n_dir), # elastic net parameter
#                      alpha_y = rep(1, n_dir),
#                      penalties = list(pen_x = rep(1, n_dir),
#                                       pen_y = rep(1, n_dir)))
```

Sometimes it is also necessary to tweak the parameters of the gradient descent algorithm, this can be done by changing the learning rate (`lr`), the learning rate decay (`lr_decay`), the maximum number of training steps (`epochs`), or the tolerance for convergence (`tol`).

Another argument that can be changes is `nearPD`. This refers to whether the computed covariance matrix should be checked for positive definiteness, and whether - in the case it is not positive definite - the nearest positive definite matrix should be used for further computations.

## Looking at the results

Similarly to PCA, CCA leads to projections of the data we can look at. The results object of the function `ccaMM()` returns the determined directions (linear combinations) in matrix `a` for `data_x` and matrix `b` for `data_y`. The rows of `a`and `b` correspond to the number of variables in each dataset, while the columns correspond to the number of directions that were computed (we can also call this the order). 
The vector `measure` returns the computed canonical correlation measure for each order. The resulting projections are returned in the matrices `phi` and `eta`, here the number of rows corresponds to the number of observations and the number of columns again corresponds to the order.

The final used penalties are returned in `pen_x` and `pen_y`, and if hyperparameter optimization for sparsity was done, a summary is returned in `summary`.

The `plot` function applied to the result object plots the values that were tested for sparsity parameters, TPO tradeoff curve, and the projections for the respective order. 
```{r}
plot(res_sparse)
```
We can also access the projections and plot them manually:

```{r}
# simple base plot
par(mfrow = c(1,2))
plot(res_classic$phi[,1], res_classic$eta[,1], 
     xlab = expression(phi), 
     ylab = expression(eta),
     main = "Order 1")
abline(c(0, res_classic$measure[1]))
plot(res_classic$phi[,2], res_classic$eta[,2], 
     xlab = expression(phi), 
     ylab = expression(eta),
     main = "Order 2")
abline(c(0, res_classic$measure[2]))
```


## Data Pre-Processing

- The algorithm expects two data matrices as input. (Not data frames, it should be matrices.)

- The data matrices should contain the variables in the columns and the observations in the rows.

- The number of observations for `data_x` and `data_y` need to be the same. Also, the observations (rows) need to be in the same order (correspond to each other).

- The number of variables can differ, and it is also possible to set a different elastic net parameter (`alpha_x` and `alpha_y`) for both sides, depending on the application.

- There is no automatic standardization performed for the input data. If applicable, this needs to be done beforehand.
